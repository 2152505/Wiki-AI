#### 1. å®éªŒç›®æ ‡

æœ¬å®éªŒç»“åˆäº†ä¸Šä¸€å®éªŒå­¦ä¹ çš„ Affine/Softmax-with-loss å±‚çš„çŸ¥è¯†æ„å»ºä¸¤å±‚ç¥ç»ç½‘ç»œå®ç°è¯¯å·®åå‘ä¼ æ’­ï¼Œå¹¶åœ¨ mnist æ‰‹å†™æ•°å­—æ•°æ®é›†ä¸Šå¯¹ä»¥ä¸Šç®—æ³•è¿›è¡Œåº”ç”¨ã€‚<br/>

#### 2. å®éªŒæ‰€ç”¨ ğ‘ƒğ‘¦ğ‘¡â„ğ‘œğ‘›

åº“åç§° ç‰ˆæœ¬ ç®€ä»‹
ğ‘ƒğ‘¦ğ‘¡â„ğ‘œğ‘› 3.6 ç¼–ç¨‹è¯­è¨€
ğ‘ğ‘¢ğ‘šğ‘ğ‘¦ 1.19.5 æ•°ç»„è¿ç®—

#### 3. å®éªŒå‰å¯¼çŸ¥è¯†

å»ºè®®æ‚¨åœ¨å­¦ä¹ å®Œæˆä¸‹åˆ—çŸ¥è¯†åå¼€å§‹æœ¬å®éªŒï¼š<br/>
python<br/>
numpy<br/>

#### 4. å®éªŒé€‚ç”¨å¯¹è±¡

æœ¬ç§‘å­¦ç”Ÿã€ç ”ç©¶ç”Ÿ <br/>
äººå·¥æ™ºèƒ½ã€ç®—æ³•ç›¸å…³ç ”ç©¶è€…ã€å¼€å‘è€…<br/>
å¤§æ•°æ®ä¸äººå·¥æ™ºèƒ½<br/>
![](./images/nmist.png)

#### æ­¥éª¤ 2ï¼šå¤šç»´æ•°ç»„æ¢¯åº¦å¦‚ä½•å®ç°

æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹åŸºäºå¾®åˆ†æ€æƒ³ä¸‹ï¼Œå¤šç»´æ•°ç»„çš„æ¢¯åº¦å¦‚ä½•å®ç°<br/>
ä¸ä¹‹å‰å®ç°ä¸€ç»´æ•°ç»„æ±‚æ¢¯åº¦çš„æ€è·¯ç›¸åŒï¼Œåªä¸è¿‡ä½¿ç”¨ numpy ä¸­ nditer è¿™ä¸ªå‡½æ•°<br/>

```

def numerical_gradient(f, x): ## nç»´æ•°ç»„æ±‚æ¢¯åº¦
    h = 1e-4 # 0.0001
    grad = np.zeros_like(x) ## æ¢¯åº¦çš„å½¢çŠ¶ä¸xçš„å½¢çŠ¶ç›¸åŒ

    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    ## multi_indexå°†å…ƒç´ ç´¢å¼•ï¼ˆ0ï¼Œ0ï¼‰ï¼ˆ0ï¼Œ1ï¼‰ç­‰å–å‡ºæ¥
    ## readwriteï¼Œä½¿ç”¨å¯è¯»å¯å†™çš„æ ¼å¼ï¼Œæˆ‘ä»¬éœ€è¦æ”¹å˜xçš„å€¼æ¥è®¡ç®—fï¼Œæ‰€ä»¥éœ€è¦å¯å†™
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx] ## å–å‡ºæŸä¸ªå…ƒç´ 
        x[idx] = float(tmp_val) + h
        fxh1 = f(x) # f(x+h)

        x[idx] = tmp_val - h
        fxh2 = f(x) # f(x-h)
        grad[idx] = (fxh1 - fxh2) / (2*h)

        x[idx] = tmp_val
        it.iternext()

    return grad
```

#### æ­¥éª¤ 3ï¼šå®ç° Affine å±‚çš„ç±»

è¿˜è®°å¾— Affine ç±»çš„å®ç°å—

æˆ‘ä»¬åœ¨ä¹‹å‰çš„åŸºç¡€ä¸Šç¨ä½œæ”¹åŠ¨

```
class Affine:
    def __init__(self, W, b):
        self.W =W
        self.b = b

        self.x = None
        self.original_x_shape = None
        self.dW = None
        self.db = None

    def forward(self, x):
        self.original_x_shape = x.shape # å¦‚æœxæ˜¯100*28*28
        x = x.reshape(x.shape[0], -1)## è¿™é‡Œçš„å½¢çŠ¶å˜æˆ100*784è¿™ç§
        self.x = x

        out = np.dot(self.x, self.W) + self.b

        return out

    def backward(self, dout):
        dx = np.dot(dout, self.W.T)
        self.dW = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis=0)

        dx = dx.reshape(*self.original_x_shape)
        return dx

```

#### æ­¥éª¤ 4ï¼šå®ç° Softmax-with-loss ç±»

è¿™é‡Œä¸ºäº†æœ‰æ›´å¥½çš„å…¼å®¹æ€§ï¼Œå†å®šä¹‰ç±»æ—¶æˆ‘ä»¬åœ¨ä¹‹å‰çš„åŸºç¡€ä¸Šåšä¸€ç‚¹å°å°çš„æ”¹åŠ¨

```
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None # softmaxè¾“å‡º
        self.t = None # æ­£ç¡®è§£

    def forward(self, x, t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error(self.y, self.t)

        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]
        if self.t.size == self.y.size:
            # å¦‚æœæ˜¯one hotæ ¼å¼
            dx = (self.y - self.t) / batch_size
        else:
            dx = self.y.copy()
            dx[np.arange(batch_size), self.t] -= 1
            dx = dx / batch_size

        return dx
```

#### æ­¥éª¤ 5ï¼šè¯¯å·®åå‘ä¼ æ’­çš„ä¸¤å±‚ç¥ç»ç½‘ç»œçš„å®ç°

æˆ‘ä»¬å·²ç»ä»‹ç»è¿‡ä¸¤å±‚ç¥ç»ç½‘ç»œäº†

åœ¨è¿™é‡Œæˆ‘ä»¬å®šä¹‰ä¸¤ç§æ±‚æ¢¯åº¦çš„æ–¹æ³•

ä¸€ç§æ˜¯ä½¿ç”¨å¾®åˆ†çš„æ€æƒ³æ±‚å‡ºæ¢¯åº¦

å¦ä¸€ä¸­æ˜¯ä½¿ç”¨è¯¯å·®åå‘ä¼ æ’­æ³•å®ç°æ¢¯åº¦

```
from collections import OrderedDict
from common.functions import *
from common.layers import *

class TwoLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):

        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)

        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])

        self.lastLayer = SoftmaxWithLoss()

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1 : t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])

        return grads

    def gradient(self, x, t):
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.lastLayer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        grads = {}
        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db
        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db

        return grads
```

å…¶ä¸­ï¼Œå¾®åˆ†æ–¹æ³•ä¸å®¹æ˜“å‡ºé”™ï¼Œä½†è®¡ç®—æ•ˆç‡è¾ƒä½
<br/>
è¯¯å·®åå‘å­¦ä¹ æ³•çš„æ¢¯åº¦è®¡ç®—æ•ˆç‡é«˜ï¼Œä½†æ˜¯å®¹æ˜“å‡ºé”™
<br/>
æˆ‘ä»¬æƒ³è¦ä½¿ç”¨è¯¯å·®åå‘å­¦ä¹ æ³•æ¥è®¡ç®—æ¢¯åº¦
<br/>
ä½†æ˜¯ä¸ºäº†é¿å…å‡ºé”™ï¼Œæˆ‘ä»¬å…ˆæ¥ç”¨å¾®åˆ†æ³•æ£€éªŒä¸€ä¸‹è¯¯å·®åå‘å­¦ä¹ æ³•è®¡ç®—çš„æ¢¯åº¦æ˜¯å¦æ­£ç¡®
<br/>

#### æ­¥éª¤ 6ï¼šè¯¯å·®åå‘å­¦ä¹ æ³•çš„æ¢¯åº¦ç¡®è®¤

æˆ‘ä»¬é€‰å‡ºå°‘é‡çš„æ•°æ®ï¼Œåˆ†åˆ«ç”¨ä¸¤ç§æ–¹æ³•è®¡ç®—ä¸€ä¸‹æ¢¯åº¦<br/>

```
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

x_batch = x_train[:3]
t_batch = t_train[:3]

grad_numerical = network.numerical_gradient(x_batch, t_batch)
grad_backprop = network.gradient(x_batch, t_batch)

for key in grad_numerical.keys():
    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )
    print(key + ":" + str(diff))
```

ä»ç»“æœçš„æ¯”å¯¹æ¥çœ‹ï¼Œæ¢¯åº¦åå‘ä¼ æ’­æ³•çš„è®¡ç®—ä¸å¾®åˆ†æ–¹æ³•çš„è®¡ç®—å·®è·å¾ˆå°<br/>

è¯´æ˜æˆ‘ä»¬çš„æ–¹æ³•æ˜¯æ­£ç¡®çš„<br/>

#### æ­¥éª¤ 7ï¼šä½¿ç”¨è¯¯å·®åå‘ä¼ æ’­æ³•å­¦ä¹ 

ä¸‹é¢æˆ‘ä»¬å°±æ˜¯ç”¨ MNIST æ•°æ®é›†è¿›è¡Œè¯¯å·®æ–¹å¸Œé‚£ä¸ªä¼ æ’­æ³•çš„å­¦ä¹ <br/>

```
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)
for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grad = network.gradient(x_batch, t_batch)

    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]

    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(train_acc, test_acc)
```

#### æ­¥éª¤ 8ï¼šæ€è€ƒä¸æ€»ç»“

è‡³æ­¤æˆ‘ä»¬ä»‹ç»äº† ReLU å±‚ã€Softmax-with-Loss å±‚ã€Affine å±‚ã€Softmax å±‚ç­‰ï¼›<br/>
è¿™äº›å±‚ä¸­å®ç°äº† forward å’Œ backward æ–¹æ³•ï¼Œé€šè¿‡å°†æ•°æ®æ­£å‘å’Œåå‘åœ°ä¼ æ’­ï¼Œå¯ä»¥é«˜æ•ˆåœ°è®¡ç®—æƒé‡å‚æ•°çš„æ¢¯åº¦ï¼›<br/>
åŒæ—¶æˆ‘ä»¬å°†å±‚è¿›è¡Œæ¨¡å—åŒ–ï¼Œç¥ç»ç½‘ç»œä¸­å¯ä»¥è‡ªç”±åœ°ç»„è£…å±‚ï¼Œè½»æ¾æ„å»ºå‡ºè‡ªå·±å–œæ¬¢çš„ç½‘ç»œã€‚<br/>
